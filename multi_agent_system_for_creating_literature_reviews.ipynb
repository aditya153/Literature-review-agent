{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf sample_data"
      ],
      "metadata": {
        "id": "3ZNgvY_IT1UV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Sb0QHtCxpJ",
        "outputId": "08636790-2bcc-49b3-8bba-b134571c69e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.5)\n",
            "Requirement already satisfied: autogen in /usr/local/lib/python3.12/dist-packages (0.10.0)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: ag2==0.10.0 in /usr/local/lib/python3.12/dist-packages (from autogen) (0.10.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (4.11.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (5.6.3)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (7.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (25.0)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (2.11.10)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (1.2.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (3.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from ag2==0.10.0->autogen) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from rouge) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.10.0->autogen) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.10.0->autogen) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.10.0->autogen) (4.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.28.1->ag2==0.10.0->autogen) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.28.1->ag2==0.10.0->autogen) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2==0.10.0->autogen) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.6.1->ag2==0.10.0->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.6.1->ag2==0.10.0->autogen) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.6.1->ag2==0.10.0->autogen) (0.4.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from docker->ag2==0.10.0->autogen) (2.32.4)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker->ag2==0.10.0->autogen) (2.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->ag2==0.10.0->autogen) (2024.11.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->docker->ag2==0.10.0->autogen) (3.4.4)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.33.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "# Install PyMuPDF\n",
        "!pip install pymupdf autogen rouge\n",
        "!pip install groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I3AOEs-IXBB"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import asyncio\n",
        "import fitz\n",
        "import time\n",
        "import re\n",
        "from groq import Groq\n",
        "import os\n",
        "from io import BytesIO\n",
        "from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
        "from rouge import Rouge\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set your API key (replace with your real key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# initialize the client\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.groq.com/openai/v1\""
      ],
      "metadata": {
        "id": "moBeQoDFFYQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYjhXnz81NKq"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from local PDF file using PyMuPDF\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"PDF processing error: {str(e)}\")\n",
        "        return \"Failed to process PDF.\"\n",
        "\n",
        "def truncate_text(text, max_words=300):\n",
        "    \"\"\"Limit text length while preserving sentence boundaries\"\"\"\n",
        "    words = text.split()[:max_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def extract_sections(text):\n",
        "    \"\"\"Identify and extract key sections from research paper text\"\"\"\n",
        "    section_patterns = {\n",
        "        'abstract': r'^\\s*(?:[\\d.]+\\s*)?Abstract\\b',\n",
        "        'introduction': r'^\\s*(?:[\\d.]+\\s*)?Introduction\\b',\n",
        "        'implementation': r'^\\s*(?:[\\d.]+\\s*)?(Implementation|Methodology)\\b'\n",
        "    }\n",
        "\n",
        "    sections = {k: '' for k in section_patterns}\n",
        "    current_section = None\n",
        "\n",
        "    for line in text.split('\\n'):\n",
        "        for section, pattern in section_patterns.items():\n",
        "            if re.search(pattern, line, re.IGNORECASE):\n",
        "                current_section = section\n",
        "                break\n",
        "        if current_section in sections:\n",
        "            sections[current_section] += line + '\\n'\n",
        "\n",
        "    combined = \"\"\n",
        "    for section in ['abstract', 'introduction', 'implementation']:\n",
        "        if sections[section]:\n",
        "            combined += f\"{section.upper()}:\\n{truncate_text(sections[section])}\\n\\n\"\n",
        "\n",
        "    return combined or truncate_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_AJwR3x1lEm"
      },
      "outputs": [],
      "source": [
        "class CustomGroupChatManager(GroupChatManager):\n",
        "    \"\"\"Custom conversation manager for agent interactions\"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.system_review = \"\"\n",
        "        self.user_review = \"\"\n",
        "\n",
        "    async def a_validate_request(self, messages, sender, config):\n",
        "        if not messages:\n",
        "            return self.groupchat.agent_by_name(\"Human\")\n",
        "        last_msg = messages[-1]\n",
        "        if last_msg[\"name\"] == \"Human\":\n",
        "            content = last_msg[\"content\"].strip().lower()\n",
        "            if content.startswith((\"revise\", \"refine\")):\n",
        "                return self.groupchat.agent_by_name(\"Reviewer\")\n",
        "        return self.groupchat.agent_by_name(\"Human\")\n",
        "\n",
        "class EnhancedEvaluationAgent(AssistantAgent):\n",
        "    \"\"\"ROUGE metric analysis agent\"\"\"\n",
        "    def __init__(self, name, *args, **kwargs):\n",
        "        super().__init__(name=name, *args, **kwargs)\n",
        "        self.rouge = Rouge()\n",
        "\n",
        "    async def analyze_reviews(self, system_review, user_review):\n",
        "        try:\n",
        "            scores = self.rouge.get_scores(user_review, system_review)[0]\n",
        "            r1 = scores['rouge-1']['f'] * 100\n",
        "            rl = scores['rouge-l']['f'] * 100\n",
        "\n",
        "            return (\n",
        "                f\"ROUGE-1 F1: {r1:.1f}%\\n\"\n",
        "                f\"ROUGE-L F1: {rl:.1f}%\\n\\n\"\n",
        "\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return f\"Analysis Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "056DoZIP1oBf"
      },
      "outputs": [],
      "source": [
        "async def process_paper(pdf_path, summarizer):\n",
        "    \"\"\"Process individual research paper\"\"\"\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    if text.startswith(\"Failed\"):\n",
        "        return None\n",
        "    try:\n",
        "        sections_text = extract_sections(text)\n",
        "        return await summarizer.a_generate_reply(\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Analyze technical content:\\n{sections_text}\\n\"\n",
        "                           \"Extract concise points for:\\n- Key contributions\\n- Methodology\\n- Technical details\\n- Novel aspects\"\n",
        "            }]\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Processing Error: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MNKjaXWk1qeq",
        "outputId": "93d4b499-f5ea-437e-85a6-671bb28e5976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload at least 6 PDF files (multiple uploads allowed)\n",
            "\n",
            "Current PDF count: 0/6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ca4e6318-41c6-41c1-b46f-002c4bbb213a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ca4e6318-41c6-41c1-b46f-002c4bbb213a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Application_of_Prompt_Engineering_in_AIGC__Taking_Stable_Diffusion_as_an_Example.pdf to Application_of_Prompt_Engineering_in_AIGC__Taking_Stable_Diffusion_as_an_Example (1).pdf\n",
            "\n",
            "Selected files:\n",
            "1. Application_of_Prompt_Engineering_in_AIGC__Taking_Stable_Diffusion_as_an_Example (1).pdf\n",
            "2. Application_of_Prompt_Engineering_in_AIGC__Taking_Stable_Diffusion_as_an_Example.pdf\n",
            "3. Does_Prompt_Engineering_Help_Turkish_Named_Entity_Recognition.pdf\n",
            "4. Enhancing_Language_Models_Through_Prompt_Engineering_-_A_Survey.pdf\n",
            "5. Optimizing_Human-AI_Interaction_Innovations_in_Prompt_Engineering.pdf\n",
            "6. Prompt_Engineering_as_Code_PEaC_an_approach_for_building_modular_reusable_and_portable_prompts.pdf\n",
            "\n",
            "Processing paper 1/6\n",
            "[autogen.oai.client: 11-01 15:19:34] {699} WARNING - Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Summarizer Agent]: Technical Summary - Application_of_Prompt_Engineering_in_AIGC__Taking_Stable_Diffusion_as_an_Example (1).pdf:\n",
            "Here are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Proposes a design framework for prompt word engineering to facilitate quick writing of prompt words for product design concepts and creative ideas.\n",
            "2. Presents a method to use Stable Diffusion to rapidly achieve creative ideas and generate product design drawings.\n",
            "3. Introduces a LoRA fine-tuning model to guide product style and combines ControlNet network to control generated content.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Utilizes Stable Diffusion as an AI painting tool to generate product design drawings based on prompt words.\n",
            "2. Employs a design framework for prompt word engineering to effectively and quickly design prompt words.\n",
            "3. Leverages LoRA fine-tuning model and ControlNet network to refine and control generated content.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Uses Stable Diffusion, a type of diffusion model, for AI painting and image generation.\n",
            "2. Employs LoRA fine-tuning model to guide product style and adapt to specific design needs.\n",
            "3. Combines ControlNet network to control and regulate generated content.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Introduces a design framework for prompt word engineering, a novel approach to facilitate quick writing of prompt words for product design.\n",
            "2. Applies LoRA fine-tuning model and ControlNet network to refine and control generated content, enhancing the specificity and accuracy of product design drawings.\n",
            "3. Demonstrates the portability of the proposed framework to mainstream AI painting platforms, such as Midjournal.\n",
            "\n",
            "Processing paper 2/6\n",
            "[autogen.oai.client: 11-01 15:19:38] {699} WARNING - Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Summarizer Agent]: Technical Summary - Application_of_Prompt_Engineering_in_AIGC__Taking_Stable_Diffusion_as_an_Example.pdf:\n",
            "Here are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Proposes a design framework for prompt word engineering to facilitate quick writing of prompt words for product design concepts and creative ideas.\n",
            "2. Presents a method to use Stable Diffusion to rapidly achieve creative ideas and generate product design drawings.\n",
            "3. Introduces a LoRA fine-tuning model to guide product style and combines ControlNet network to control generated content.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Utilizes Stable Diffusion as an AI painting tool to generate product design drawings based on prompt words.\n",
            "2. Employs a design framework for prompt word engineering to effectively and quickly design prompt words.\n",
            "3. Leverages LoRA fine-tuning model and ControlNet network to refine and control generated content.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Uses Stable Diffusion, a type of diffusion model, for AI painting and image generation.\n",
            "2. Employs LoRA fine-tuning model to guide product style and adapt to specific design needs.\n",
            "3. Combines ControlNet network to control and regulate generated content.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Introduces a design framework for prompt word engineering, a novel approach to facilitate quick writing of prompt words for product design.\n",
            "2. Applies LoRA fine-tuning model and ControlNet network to refine and control generated content, enhancing the specificity and accuracy of product design drawings.\n",
            "3. Demonstrates the portability of the proposed framework to mainstream AI painting platforms, such as Midjournal.\n",
            "\n",
            "Processing paper 3/6\n",
            "[autogen.oai.client: 11-01 15:19:42] {699} WARNING - Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Summarizer Agent]: Technical Summary - Does_Prompt_Engineering_Help_Turkish_Named_Entity_Recognition.pdf:\n",
            "Here are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Investigated prompt engineering for Turkish named entity recognition (NER) using pre-trained language models.\n",
            "2. Studied two prompting strategies (zero-shot and few-shot) to guide language models towards correct entity identification.\n",
            "3. Demonstrated that carefully constructed prompts can achieve high accuracy on entity recognition in languages with complex morphology.\n",
            "4. Evaluated the impact of prompt templates on NER performance.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Formulated NER as a machine reading comprehension (MRC) task.\n",
            "2. Used pre-trained large language models (LLMs) to comprehend text and answer questions about entity types.\n",
            "3. Constructed prompts to elicit desired entity mentions and their types from LLMs.\n",
            "4. Conducted experiments on two large language models using different prompt templates.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Utilized transformer-based learning systems, specifically pre-trained LLMs like BERT and GPT.\n",
            "2. Employed MRC setting to tackle challenges of sequence labeling-based NER approaches, such as nested entity mentions.\n",
            "3. Formed queries for each entity type and returned answer fragments from LLMs.\n",
            "4. Evaluated performance using different prompt templates.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Applied MRC formulation to NER in Turkish, a language with complex morphology.\n",
            "2. Investigated prompt engineering for NER in a low-resource language setting.\n",
            "3. Explored zero-shot and few-shot prompting strategies for NER.\n",
            "4. Demonstrated the effectiveness of carefully constructed prompts for NER in languages with complex morphology.\n",
            "\n",
            "Processing paper 4/6\n",
            "[autogen.oai.client: 11-01 15:19:46] {699} WARNING - Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Summarizer Agent]: Technical Summary - Enhancing_Language_Models_Through_Prompt_Engineering_-_A_Survey.pdf:\n",
            "Here are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. The article highlights the role of prompt engineering in enhancing the performance and reliability of language models.\n",
            "2. It addresses the challenges and limitations of current language models, such as bias, lack of commonsense reasoning, and contextual understanding.\n",
            "3. The article aims to provide an overview of prompt engineering techniques and their potential impact on language models.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. The article provides an overview of language models, their working, and their applications.\n",
            "2. It analyzes the challenges and limitations of current language models.\n",
            "3. The article discusses the concept of prompt engineering and its techniques.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Language models learn patterns and structures from large amounts of text data to generate human-like text.\n",
            "2. They use statistical techniques to understand relationships between words and sentences.\n",
            "3. Language models aim to learn the joint probability distribution of sequences of words in a language.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. The article introduces prompt engineering as a promising approach to address the challenges of language models.\n",
            "2. It highlights the potential impact of prompt engineering on enhancing the performance and reliability of language models.\n",
            "3. The article aims to provide a comprehensive overview of prompt engineering techniques and their applications. \n",
            "\n",
            "Let me know if I can help with anything else!\n",
            "\n",
            "Processing paper 5/6\n",
            "[autogen.oai.client: 11-01 15:19:49] {699} WARNING - Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Summarizer Agent]: Technical Summary - Optimizing_Human-AI_Interaction_Innovations_in_Prompt_Engineering.pdf:\n",
            "Here are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. A structured methodology for prompt generation to optimize interactions with large language models (LLMs).\n",
            "2. Identification of best practices for prompt design, emphasizing clarity, specificity, and contextualization.\n",
            "3. Findings indicate that tailored prompts can significantly improve accuracy across various applications.\n",
            "4. Advocacy for the integration of prompt engineering in high-stakes environments to promote effective and ethical communication with AI systems.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Comprehensive review of existing research studies on prompt engineering.\n",
            "2. Analysis of the role of prompt engineering in enhancing the quality and relevance of AI-generated outputs.\n",
            "3. Examination of the challenges faced in maintaining consistency and reliability in AI responses.\n",
            "4. Discussion of various prompt engineering methods, including Automatic Prompt Engineer and synthetic prompting.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Use of large language models (LLMs) such as ChatGPT for prompt generation and optimization.\n",
            "2. Importance of clarity, specificity, and contextualization in prompt design.\n",
            "3. Application of prompt engineering in various domains, including healthcare, education, and customer service.\n",
            "4. Discussion of scoring functions, such as Domain Condition Pointwise Mutual Information, for addressing surface competition in LLM models.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Introduction of Automatic Prompt Engineer, an algorithm that uses LLM models to automatically generate and optimize prompts.\n",
            "2. Proposal of synthetic prompting, a method that uses LLM models to generate additional examples from a limited set of seed examples.\n",
            "3. Discussion of prompt-to-prompt generation methodology for enhancing zero-shot learning.\n",
            "4. Identification of the need for standardized guidelines in prompt engineering to address challenges in maintaining consistency and reliability in AI responses.\n",
            "\n",
            "Processing paper 6/6\n",
            "[autogen.oai.client: 11-01 15:19:53] {699} WARNING - Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Summarizer Agent]: Technical Summary - Prompt_Engineering_as_Code_PEaC_an_approach_for_building_modular_reusable_and_portable_prompts.pdf:\n",
            "Here are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Introduces Prompt Engineering as Code (PEaC), a novel approach to organizing prompts in a modular, reusable, and portable way.\n",
            "2. Enables efficient and scalable prompt systems through a human-readable data serialization language.\n",
            "3. Increases prompt reusability, reduces redundancy, and promotes adaptability of prompt systems.\n",
            "4. Represents progress in establishing standardized and scalable engineered prompts.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Uses a syntax language to assemble prompts as modular components, similar to conventional programming languages.\n",
            "2. Employs a human-readable data serialization language to manage prompts.\n",
            "3. Evaluates the methodology through implementation in multiple LLM-driven applications.\n",
            "4. Assesses enhancements in prompt management and adaptability.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Utilizes Infrastructure as Code (IaC) principles to manage prompts.\n",
            "2. Designs a syntax language for modular prompt assembly.\n",
            "3. Leverages data serialization for human-readable prompt configuration.\n",
            "4. Implements PEaC in various LLM-driven applications.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Applies IaC principles to prompt engineering, enabling modularity, reusability, and portability.\n",
            "2. Introduces a syntax language for assembling prompts as modular components.\n",
            "3. Uses data serialization for efficient and scalable prompt management.\n",
            "4. Provides a standardized approach to engineered prompts, promoting adaptability and reusability.\n",
            "Human (to chat_manager):\n",
            "\n",
            "TECHNICAL SUMMARIES:\n",
            "Here are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Proposes a design framework for prompt word engineering to facilitate quick writing of prompt words for product design concepts and creative ideas.\n",
            "2. Presents a method to use Stable Diffusion to rapidly achieve creative ideas and generate product design drawings.\n",
            "3. Introduces a LoRA fine-tuning model to guide product style and combines ControlNet network to control generated content.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Utilizes Stable Diffusion as an AI painting tool to generate product design drawings based on prompt words.\n",
            "2. Employs a design framework for prompt word engineering to effectively and quickly design prompt words.\n",
            "3. Leverages LoRA fine-tuning model and ControlNet network to refine and control generated content.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Uses Stable Diffusion, a type of diffusion model, for AI painting and image generation.\n",
            "2. Employs LoRA fine-tuning model to guide product style and adapt to specific design needs.\n",
            "3. Combines ControlNet network to control and regulate generated content.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Introduces a design framework for prompt word engineering, a novel approach to facilitate quick writing of prompt words for product design.\n",
            "2. Applies LoRA fine-tuning model and ControlNet network to refine and control generated content, enhancing the specificity and accuracy of product design drawings.\n",
            "3. Demonstrates the portability of the proposed framework to mainstream AI painting platforms, such as Midjournal.nHere are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Proposes a design framework for prompt word engineering to facilitate quick writing of prompt words for product design concepts and creative ideas.\n",
            "2. Presents a method to use Stable Diffusion to rapidly achieve creative ideas and generate product design drawings.\n",
            "3. Introduces a LoRA fine-tuning model to guide product style and combines ControlNet network to control generated content.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Utilizes Stable Diffusion as an AI painting tool to generate product design drawings based on prompt words.\n",
            "2. Employs a design framework for prompt word engineering to effectively and quickly design prompt words.\n",
            "3. Leverages LoRA fine-tuning model and ControlNet network to refine and control generated content.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Uses Stable Diffusion, a type of diffusion model, for AI painting and image generation.\n",
            "2. Employs LoRA fine-tuning model to guide product style and adapt to specific design needs.\n",
            "3. Combines ControlNet network to control and regulate generated content.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Introduces a design framework for prompt word engineering, a novel approach to facilitate quick writing of prompt words for product design.\n",
            "2. Applies LoRA fine-tuning model and ControlNet network to refine and control generated content, enhancing the specificity and accuracy of product design drawings.\n",
            "3. Demonstrates the portability of the proposed framework to mainstream AI painting platforms, such as Midjournal.nHere are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Investigated prompt engineering for Turkish named entity recognition (NER) using pre-trained language models.\n",
            "2. Studied two prompting strategies (zero-shot and few-shot) to guide language models towards correct entity identification.\n",
            "3. Demonstrated that carefully constructed prompts can achieve high accuracy on entity recognition in languages with complex morphology.\n",
            "4. Evaluated the impact of prompt templates on NER performance.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Formulated NER as a machine reading comprehension (MRC) task.\n",
            "2. Used pre-trained large language models (LLMs) to comprehend text and answer questions about entity types.\n",
            "3. Constructed prompts to elicit desired entity mentions and their types from LLMs.\n",
            "4. Conducted experiments on two large language models using different prompt templates.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Utilized transformer-based learning systems, specifically pre-trained LLMs like BERT and GPT.\n",
            "2. Employed MRC setting to tackle challenges of sequence labeling-based NER approaches, such as nested entity mentions.\n",
            "3. Formed queries for each entity type and returned answer fragments from LLMs.\n",
            "4. Evaluated performance using different prompt templates.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Applied MRC formulation to NER in Turkish, a language with complex morphology.\n",
            "2. Investigated prompt engineering for NER in a low-resource language setting.\n",
            "3. Explored zero-shot and few-shot prompting strategies for NER.\n",
            "4. Demonstrated the effectiveness of carefully constructed prompts for NER in languages with complex morphology.nHere are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. The article highlights the role of prompt engineering in enhancing the performance and reliability of language models.\n",
            "2. It addresses the challenges and limitations of current language models, such as bias, lack of commonsense reasoning, and contextual understanding.\n",
            "3. The article aims to provide an overview of prompt engineering techniques and their potential impact on language models.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. The article provides an overview of language models, their working, and their applications.\n",
            "2. It analyzes the challenges and limitations of current language models.\n",
            "3. The article discusses the concept of prompt engineering and its techniques.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Language models learn patterns and structures from large amounts of text data to generate human-like text.\n",
            "2. They use statistical techniques to understand relationships between words and sentences.\n",
            "3. Language models aim to learn the joint probability distribution of sequences of words in a language.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. The article introduces prompt engineering as a promising approach to address the challenges of language models.\n",
            "2. It highlights the potential impact of prompt engineering on enhancing the performance and reliability of language models.\n",
            "3. The article aims to provide a comprehensive overview of prompt engineering techniques and their applications. \n",
            "\n",
            "Let me know if I can help with anything else!nHere are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. A structured methodology for prompt generation to optimize interactions with large language models (LLMs).\n",
            "2. Identification of best practices for prompt design, emphasizing clarity, specificity, and contextualization.\n",
            "3. Findings indicate that tailored prompts can significantly improve accuracy across various applications.\n",
            "4. Advocacy for the integration of prompt engineering in high-stakes environments to promote effective and ethical communication with AI systems.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Comprehensive review of existing research studies on prompt engineering.\n",
            "2. Analysis of the role of prompt engineering in enhancing the quality and relevance of AI-generated outputs.\n",
            "3. Examination of the challenges faced in maintaining consistency and reliability in AI responses.\n",
            "4. Discussion of various prompt engineering methods, including Automatic Prompt Engineer and synthetic prompting.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Use of large language models (LLMs) such as ChatGPT for prompt generation and optimization.\n",
            "2. Importance of clarity, specificity, and contextualization in prompt design.\n",
            "3. Application of prompt engineering in various domains, including healthcare, education, and customer service.\n",
            "4. Discussion of scoring functions, such as Domain Condition Pointwise Mutual Information, for addressing surface competition in LLM models.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Introduction of Automatic Prompt Engineer, an algorithm that uses LLM models to automatically generate and optimize prompts.\n",
            "2. Proposal of synthetic prompting, a method that uses LLM models to generate additional examples from a limited set of seed examples.\n",
            "3. Discussion of prompt-to-prompt generation methodology for enhancing zero-shot learning.\n",
            "4. Identification of the need for standardized guidelines in prompt engineering to address challenges in maintaining consistency and reliability in AI responses.nHere are the extracted points:\n",
            "\n",
            "**Key Contributions:**\n",
            "\n",
            "1. Introduces Prompt Engineering as Code (PEaC), a novel approach to organizing prompts in a modular, reusable, and portable way.\n",
            "2. Enables efficient and scalable prompt systems through a human-readable data serialization language.\n",
            "3. Increases prompt reusability, reduces redundancy, and promotes adaptability of prompt systems.\n",
            "4. Represents progress in establishing standardized and scalable engineered prompts.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. Uses a syntax language to assemble prompts as modular components, similar to conventional programming languages.\n",
            "2. Employs a human-readable data serialization language to manage prompts.\n",
            "3. Evaluates the methodology through implementation in multiple LLM-driven applications.\n",
            "4. Assesses enhancements in prompt management and adaptability.\n",
            "\n",
            "**Technical Details:**\n",
            "\n",
            "1. Utilizes Infrastructure as Code (IaC) principles to manage prompts.\n",
            "2. Designs a syntax language for modular prompt assembly.\n",
            "3. Leverages data serialization for human-readable prompt configuration.\n",
            "4. Implements PEaC in various LLM-driven applications.\n",
            "\n",
            "**Novel Aspects:**\n",
            "\n",
            "1. Applies IaC principles to prompt engineering, enabling modularity, reusability, and portability.\n",
            "2. Introduces a syntax language for assembling prompts as modular components.\n",
            "3. Uses data serialization for efficient and scalable prompt management.\n",
            "4. Provides a standardized approach to engineered prompts, promoting adaptability and reusability.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.agentchat.groupchat:GroupChat is underpopulated with 2 agents. Consider setting speaker_selection_method to 'round_robin' or allow_repeat_speaker to False, or use direct communication, unless repeated speaker is desired.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 11-01 15:19:57] {699} WARNING - Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Next speaker: Reviewer\n",
            "\n",
            "[autogen.oai.client: 11-01 15:19:58] {699} WARNING - Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model meta-llama/llama-4-scout-17b-16e-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviewer (to chat_manager):\n",
            "\n",
            "Here is a 500-word literature review based on the provided points:\n",
            "\n",
            "The rapidly evolving field of artificial intelligence (AI) has witnessed significant advancements in natural language processing (NLP) and language models. A crucial aspect of these developments is prompt engineering, which involves designing and optimizing prompts to interact with language models effectively. This literature review aims to provide a comprehensive overview of the key contributions, methodologies, and technical details of prompt engineering.\n",
            "\n",
            "Recent studies have highlighted the importance of prompt engineering in enhancing the performance and reliability of language models. For instance, a study proposed a design framework for prompt word engineering to facilitate quick writing of prompt words for product design concepts and creative ideas (1). Another study introduced a method to use Stable Diffusion to rapidly achieve creative ideas and generate product design drawings (2). These approaches demonstrate the potential of prompt engineering in optimizing interactions with language models.\n",
            "\n",
            "The methodology of prompt engineering involves designing and constructing prompts to elicit desired responses from language models. Researchers have employed various techniques, such as zero-shot and few-shot prompting strategies, to guide language models towards correct entity identification (3). Additionally, studies have utilized transformer-based learning systems, specifically pre-trained language models like BERT and GPT, to comprehend text and answer questions about entity types (4).\n",
            "\n",
            "Technical details of prompt engineering include the use of large language models (LLMs) such as ChatGPT for prompt generation and optimization (5). The importance of clarity, specificity, and contextualization in prompt design has also been emphasized (6). Furthermore, researchers have discussed the application of prompt engineering in various domains, including healthcare, education, and customer service (7).\n",
            "\n",
            "A novel aspect of prompt engineering is the introduction of structured methodologies for prompt generation, such as Prompt Engineering as Code (PEaC) (8). PEaC enables efficient and scalable prompt systems through a human-readable data serialization language, promoting adaptability and reusability of prompt systems. Another approach, Automatic Prompt Engineer, uses LLM models to automatically generate and optimize prompts (9).\n",
            "\n",
            "The findings of these studies indicate that tailored prompts can significantly improve accuracy across various applications. Moreover, the integration of prompt engineering in high-stakes environments can promote effective and ethical communication with AI systems (10). However, challenges persist in maintaining consistency and reliability in AI responses, highlighting the need for standardized guidelines in prompt engineering.\n",
            "\n",
            "In conclusion, prompt engineering has emerged as a crucial aspect of AI research, with significant contributions to optimizing interactions with language models. The methodologies, technical details, and novel aspects of prompt engineering discussed in this review demonstrate its potential to enhance the performance and reliability of language models. As AI continues to evolve, the importance of prompt engineering will only continue to grow, emphasizing the need for continued research and development in this area.\n",
            "\n",
            "References:\n",
            "\n",
            "(1) Technical Summaries, Key Contributions, 1.\n",
            "\n",
            "(2) Technical Summaries, Key Contributions, 2.\n",
            "\n",
            "(3) Key Contributions, 2.\n",
            "\n",
            "(4) Technical Details, 1.\n",
            "\n",
            "(5) Technical Details, 2.\n",
            "\n",
            "(6) Methodology, 2.\n",
            "\n",
            "(7) Technical Details, 3.\n",
            "\n",
            "(8) Key Contributions, 1.\n",
            "\n",
            "(9) Novel Aspects, 1.\n",
            "\n",
            "(10) Key Contributions, 4.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> TERMINATING RUN (94c8fa67-cd4b-49ca-a8d8-4e83b80b6bc8): Maximum rounds (2) reached\n",
            "\n",
            "Command (evaluate/revise/exit): evaluate \n",
            "Paste your review (END to finish):\n",
            "The rapidly evolving field of artificial intelligence (AI) has witnessed significant advancements in natural language processing (NLP) and language models. A crucial aspect of these developments is prompt engineering, which involves designing and optimizing prompts to interact with language models effectively. This literature review aims to provide a comprehensive overview of the key contributions, methodologies, and technical details of prompt engineering.  Recent studies have highlighted the importance of prompt engineering in enhancing the performance and reliability of language models. For instance, a study proposed a design framework for prompt word engineering to facilitate quick writing of prompt words for product design concepts and creative ideas (1). Another study introduced a method to use Stable Diffusion to rapidly achieve creative ideas and generate product design drawings (2). These approaches demonstrate the potential of prompt engineering in optimizing interactions with language models.  The methodology of prompt engineering involves designing and constructing prompts to elicit desired responses from language models. Researchers have employed various techniques, such as zero-shot and few-shot prompting strategies, to guide language models towards correct entity identification (3). Additionally, studies have utilized transformer-based learning systems, specifically pre-trained language models like BERT and GPT, to comprehend text and answer questions about entity types (4).  Technical details of prompt engineering include the use of large language models (LLMs) such as ChatGPT for prompt generation and optimization (5). The importance of clarity, specificity, and contextualization in prompt design has also been emphasized (6). Furthermore, researchers have discussed the application of prompt engineering in various domains, including healthcare, education, and customer service (7).  A novel aspect of prompt engineering is the introduction of structured methodologies for prompt generation, such as Prompt Engineering as Code (PEaC) (8). PEaC enables efficient and scalable prompt systems through a human-readable data serialization language, promoting adaptability and reusability of prompt systems. Another approach, Automatic Prompt Engineer, uses LLM models to automatically generate and optimize prompts (9).  The findings of these studies indicate that tailored prompts can significantly improve accuracy across various applications. Moreover, the integration of prompt engineering in high-stakes environments can promote effective and ethical communication with AI systems (10). However, challenges persist in maintaining consistency and reliability in AI responses, highlighting the need for standardized guidelines in prompt engineering.  In conclusion, prompt engineering has emerged as a crucial aspect of AI research, with significant contributions to optimizing interactions with language models. The methodologies, technical details, and novel aspects of prompt engineering discussed in this review demonstrate its potential to enhance the performance and reliability of language models. As AI continues to evolve, the importance of prompt engineering will only continue to grow, emphasizing the need for continued research and development in this area.\n",
            "END\n",
            "ROUGE-1 F1: 96.4%\n",
            "ROUGE-L F1: 96.4%\n",
            "\n",
            "\n",
            "\n",
            "Command (evaluate/revise/exit): exit\n",
            "Analysis completed\n"
          ]
        }
      ],
      "source": [
        "async def run_analysis():\n",
        "    \"\"\"Main execution workflow\"\"\"\n",
        "    content_dir = '/content'\n",
        "    if not os.path.exists(content_dir):\n",
        "        os.makedirs(content_dir)\n",
        "\n",
        "    pdf_files = []\n",
        "    print(\"Upload at least 6 PDF files (multiple uploads allowed)\")\n",
        "\n",
        "    while len(pdf_files) < 6:\n",
        "        print(f\"\\nCurrent PDF count: {len(pdf_files)}/6\")\n",
        "        if len(pdf_files) > 0:\n",
        "            print(\"Please upload more files to reach minimum 6\")\n",
        "        files.upload()\n",
        "        pdf_files = sorted([\n",
        "            f for f in os.listdir(content_dir)\n",
        "            if f.lower().endswith('.pdf')\n",
        "        ])[:6]\n",
        "\n",
        "    pdf_paths = [os.path.join(content_dir, f) for f in pdf_files[:6]]\n",
        "    print(\"\\nSelected files:\")\n",
        "    for i, path in enumerate(pdf_paths):\n",
        "        print(f\"{i+1}. {os.path.basename(path)}\")\n",
        "\n",
        "    # Configure LLM agents\n",
        "    llm_config = {\n",
        "        \"config_list\": [{\n",
        "            \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            \"temperature\": 0.3,\n",
        "            \"max_tokens\": 1000\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    summarizer = AssistantAgent(\n",
        "        name=\"Summarizer\",\n",
        "        system_message=\"Extract 3-4 concise technical points per category:\\n\"\n",
        "                       \"- Key contributions\\n- Methodology\\n- Technical details\\n- Novel aspects\",\n",
        "        llm_config=llm_config\n",
        "    )\n",
        "\n",
        "    reviewer = AssistantAgent(\n",
        "        name=\"Reviewer\",\n",
        "        system_message=\"Generate cohesive literature review of exact 500 words not less than that.\",\n",
        "        llm_config=llm_config\n",
        "    )\n",
        "\n",
        "    human = UserProxyAgent(\n",
        "        name=\"Human\",\n",
        "        human_input_mode=\"ALWAYS\",\n",
        "        max_consecutive_auto_reply=1,\n",
        "        code_execution_config={\"use_docker\": False}\n",
        "    )\n",
        "\n",
        "    manager = CustomGroupChatManager(\n",
        "        groupchat=GroupChat(\n",
        "            agents=[human, reviewer],\n",
        "            messages=[],\n",
        "            max_round=2\n",
        "        ),\n",
        "        llm_config=llm_config\n",
        "    )\n",
        "\n",
        "    # Process papers\n",
        "    summaries = []\n",
        "    for idx, path in enumerate(pdf_paths):\n",
        "        print(f\"\\nProcessing paper {idx+1}/{len(pdf_paths)}\")\n",
        "        paper_title = os.path.basename(path)\n",
        "        if summary := await process_paper(path, summarizer):\n",
        "            print(f\"\\n[Summarizer Agent]: Technical Summary - {paper_title}:\")\n",
        "            print(summary)\n",
        "            summaries.append(summary)\n",
        "        time.sleep(3)\n",
        "\n",
        "    # Generate literature review\n",
        "    await human.a_send(\n",
        "        message=f\"TECHNICAL SUMMARIES:\\n{'n'.join(summaries)}\",\n",
        "        recipient=manager,\n",
        "        request_reply=True\n",
        "    )\n",
        "    manager.system_review = next(\n",
        "        (msg[\"content\"] for msg in manager.groupchat.messages\n",
        "         if msg[\"name\"] == \"Reviewer\"), \"\"\n",
        "    )\n",
        "\n",
        "    # Interactive refinement\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nCommand (evaluate/revise/exit): \").strip().lower()\n",
        "            if not user_input:\n",
        "                continue\n",
        "            if user_input.startswith(\"exit\"):\n",
        "                break\n",
        "            elif user_input.startswith(\"evaluate\"):\n",
        "                print(\"Paste your review (END to finish):\")\n",
        "                user_review = \"\\n\".join(iter(input, 'END'))\n",
        "                if user_review:\n",
        "                    evaluator = EnhancedEvaluationAgent(\n",
        "                        name=\"Evaluator\",\n",
        "                        llm_config=llm_config\n",
        "                    )\n",
        "                    rouge_scores = await evaluator.analyze_reviews(\n",
        "                        manager.system_review,\n",
        "                        user_review\n",
        "                    )\n",
        "                    print(rouge_scores)\n",
        "            elif user_input.startswith((\"revise\", \"refine\")):\n",
        "                await human.a_send(user_input, recipient=manager, request_reply=True)\n",
        "                manager.system_review = next(\n",
        "                    (msg[\"content\"] for msg in manager.groupchat.messages\n",
        "                     if msg[\"name\"] == \"Reviewer\"), \"\"\n",
        "                )\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"nSession terminated\")\n",
        "            break\n",
        "    print(\"Analysis completed\")\n",
        "\n",
        "# Run the main workflow\n",
        "await run_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RbJ44qVCfhS",
        "outputId": "26918fe5-f692-428a-9212-2cc0ccba7630"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git rm -r --cached .\n",
        "!git add \"multi-agent-system-for-creating-literature-reviews.ipynb\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCZFUxf3IkBN",
        "outputId": "863404c8-d360-475b-d0f8-a362be5f6849"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: pathspec '.' did not match any files\n",
            "fatal: pathspec 'multi-agent-system-for-creating-literature-reviews.ipynb' did not match any files\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}